{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2259f414-682d-4790-be41-14daff124247",
   "metadata": {},
   "source": [
    "1ans:\n",
    "\n",
    "Anomaly detection is the process of identifying rare events, patterns or outliers in a dataset that deviate from the expected behavior or normal range of variation. The purpose of anomaly detection is to identify these anomalies, which could be indicative of potential problems, errors, fraud, or security threats in a system or dataset.\n",
    "\n",
    "2ans:\n",
    "\n",
    "Lack of labeled data: In many cases, labeled data  may not be available, making it difficult to train supervised learning models. This can require the use of unsupervised learning techniques or other methods that can handle unlabeled data.\n",
    "\n",
    "High dimensionality: Many datasets have a large number of features, making it difficult to identify anomalies in high-dimensional space. This can require dimensionality reduction techniques or other approaches that can effectively handle high-dimensional data.\n",
    "\n",
    "Imbalanced data: In some datasets, anomalies may be rare compared to normal data, leading to imbalanced datasets. This can cause traditional machine learning algorithms to bias towards the majority class, making it difficult to identify the anomalies.\n",
    "\n",
    "3ans:\n",
    "\n",
    "Unsupervised anomaly detection does not require labeled data and relies on identifying patterns and outliers in the data that deviate from the normal behavior or expected range of variation. Unsupervised anomaly detection algorithms can include clustering-based methods, density-based methods, and distance-based methods.\n",
    "\n",
    "supervised anomaly detection relies on labeled data to identify anomalies in the data. In supervised anomaly detection, the algorithm is trained on a labeled dataset, where the normal and anomalous data points are pre-labeled.\n",
    "\n",
    "4ans:\n",
    "\n",
    "Statistical methods:  These methods assume that the normal behavior of a system or dataset follows a certain probability distribution, such as a normal distribution or Poisson distribution. Statistical methods can include Z-score, Mahalanobis distance, and percentile-based methods.\n",
    "\n",
    "Machine learning algorithms: Machine learning algorithms use supervised or unsupervised techniques to learn patterns and behaviors in the data and identify anomalies. These algorithms can include decision trees, random forests, support vector machines, clustering-based methods, and density-based methods.\n",
    "\n",
    "Deep learning models: Deep learning models use neural networks to learn complex patterns and behaviors in high-dimensional data. These models can include autoencoders, recurrent neural networks, and convolutional neural networks.\n",
    "\n",
    "5ans:\n",
    "\n",
    "1.The data follows a multivariate normal distribution: Distance-based methods such as the Mahalanobis distance assume that the data follows a multivariate normal distribution. This assumption is important as it allows for the calculation of distances between data points based on the covariance structure of the data.\n",
    "\n",
    "2.The normal data points are densely packed: This assumption allows for the identification of outliers based on their distance from the dense regions of the data.\n",
    "\n",
    "3.The feature space is Euclidean: Distance-based methods assume that the feature space is Euclidean, which means that distances between data points can be calculated using the Euclidean distance formula. \n",
    "\n",
    "4.The dataset is balanced: Distance-based methods assume that the dataset is balanced, meaning that the number of normal and anomalous data points is roughly equal. \n",
    "\n",
    "6ans:\n",
    "\n",
    "The Local Outlier Factor (LOF) algorithm computes anomaly scores based on the degree of local deviation of a data point from its neighbors. The algorithm computes a score for each data point in the dataset, with higher scores indicating a higher likelihood of being an anomaly.\n",
    "\n",
    "7ans:\n",
    "\n",
    "n_estimators: The number of trees in the isolation forest. Increasing the number of trees can improve the accuracy of the algorithm but can also increase the computational complexity and memory usage.\n",
    "\n",
    "max_samples: The number of samples to draw from the data when constructing each tree. The algorithm selects a random subset of max_samples from the dataset to build each tree, which can reduce the impact of outliers on the results. Increasing max_samples can improve the accuracy of the algorithm but can also increase the computational complexity.\n",
    "\n",
    "8ans:\n",
    "\n",
    "n this case, the data point has only 2 neighbors of the same class within a radius of 0.5, and KNN with K=10 means we need to find the 10 nearest neighbors of the data point. Since there are only 2 neighbors within a radius of 0.5, we cannot find 10 nearest neighbors and therefore cannot compute the LOF.\n",
    "\n",
    "9ans:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
